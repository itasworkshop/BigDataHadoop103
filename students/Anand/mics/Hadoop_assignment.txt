1. What is Default replication factor and how will you change it at file level?
Ans-The default replication factor is 3 and we change in hdfs-site.xml file by giving following property and value
ex.<property>
<name>dfs.replication</name>
<value>1</value>
</property>
 
2. Why do we need replication factor > 1 in production Hadoop cluster?
Ans.-As HDFS is designed to be Fault Tolerance. To keep up with this feature, HDFS replicates the data sent to it for storage so that the data is available. But how will the data be available by just creating replicas? HDFS ensures that the replicas are stored in such a way that if one DataNodes fails, the data is available in another node. For this, HDFS needs to store copy of data in different node.But what if the entire Rack fails? For this, HDFS needs to keep data in another Rack. Hence, ideally HDFS stores one copy of block of data in one node of one Rack and other two copies in different nodes of different Rack. This ensures Fault tolerance and High availability.

3. How will you combine the 4 part-r files of a mapreduce job?
Ans.-To merge these output files into a single file in HDFS, One way is to use a single Reducer, else, we can create an MR job with a set of mappers and a single reducer.
We can also merge by using the below options:
hdfs dfs -getmerge <source-dir-on-hdfs> <dest-dir-on-localfs>
or
hdfs dfs -cat <source-dir-on-hdfs/part-r*> >> CombinerResult.txt
 
4. What are the Compression techniques in HDFS and which is the best one and why?
Ans.- 1.GZIP
 2.BZIP2
 3.LZO
 4.SNAPPY

5. How will you view the compressed files via HDFS command?
Ans.- Step 1: Copy any compressed file to your hdfs dir: 
 hadoop fs -put logs.tar.gz /tmp/
 Step 2: Now you can use in-build hdfs text command to read this .gz file. This command-line will automatically find the right decompressor for any simple text file and print the uncompressed data to standard output

6. What is Secondary Namenode and its Functionalities? why do we need it?
Ans.-It gets the edit logs from the namenode in regular intervals and applies to fsimage,  Once it has new fsimage, it copies back to namenode. Namenode will use this fsimage for the next restart, which will reduce the startup time Secondary Namenode whole purpose is to have a checkpoint in HDFS. Its just a helper node for namenode. That’s why it also known as checkpoint node inside the community.
 
7. What is Backup node and how is it different from Secondary namenode?
Ans.-Backup node keeps a copy of the namespace in main memory similar to NameNode, its main memory (hardware) specifications should be same as the NameNode. Backup node does not need to download fsimage and edits files from the active NameNode to create a checkpoint, as it already has an up-to-date state of the namespace in it’s own main memory. So, creating checkpoint in backup node is just saving a copy of file system meta-data (namespace) from main-memory to its local files system.
 
8. What is FSimage and editlogs and how they are related?
Ans.- FsImage is a file stored on the OS filesystem that contains the complete directory structure (namespace) of the HDFS with details about the location of the data on the Data Blocks and which blocks are stored on which node.
EditLogs is a transaction log that record  the changes in the HDFS file system or any action performed on the HDFS cluster such as addtion of a new block, replication, deletion etc., It records the changes since the last FsImage was created, it then merges the changes into the FsImage file to create a new FsImage file.
9. what is default block size in HDFS? and why is it so large?
 Ans.-The default size of a block in HDFS is 128 MB (Hadoop 2.x) and 64 MB (Hadoop 1.x) which is much larger as compared to the Linux system where the block size is 4KB. The reason of having this huge block size is to minimize the cost of seek and reduce the meta data information generated per block.
10. How will you copy a large file of 50GB into HDFS in parllel
Ans.- As hadoop does it automatically for user does not worry about it as we give all the details like replication factor.
11. what is Balancing in HDFS?
Ans.-The HDFS Balancer re-balances data across the DataNodes, moving blocks from over-utilized to under-utilized nodes. HDFS data might not always be distributed uniformly across DataNodes.The reason for non-uniform distribution is the addition of new DataNodes to an existing cluster.
12. What is expunge in HDFS ?
Ans.-This command is used to empty the trash available in an HDFS system.

